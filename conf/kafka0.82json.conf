# *************************************csvKafka flume **********************************

# define source channels sinks
csvKafka.sources = sourceSpool
csvKafka.channels = cMem1
csvKafka.sinks = sk1

# define source
csvKafka.sources.sourceSpool.type = com.act.maxc.flume.sources.file.SpoolDirectorySource
csvKafka.sources.sourceSpool.spoolDir = /u01/liuming/testDir/test2
csvKafka.sources.sourceSpool.includePattern = ((csv|avro)$)
csvKafka.sources.sourceSpool.ignorePattern = ((tmp)$)
csvKafka.sources.sourceSpool.maxBackoff = 10000
csvKafka.sources.sourceSpool.batchSize = 100000
csvKafka.sources.sourceSpool.deserializer = com.act.maxc.flume.sources.file.deserializers.SchemaFileCsvDeserializer$Builder
csvKafka.sources.sourceSpool.deserializer.schemaFilePath = /home/avroSchema/test.avro
csvKafka.sources.sourceSpool.deserializer.splitRegex = ,
csvKafka.sources.sourceSpool.selector.type = replicating
csvKafka.sources.sourceSpool.channels = cMem1

# define HDFS channel
csvKafka.channels.cMem1.type = memory
csvKafka.channels.cMem1.capacity = 10000000
csvKafka.channels.cMem1.transactionCapacity = 10000000
csvKafka.channels.cMem1.byteCapacityBufferPercentage = 10
csvKafka.channels.cMem1.byteCapacity = 2000000000
csvKafka.channels.cMem1.keep-alive = 10

#  define  sinks
csvKafka.sinks.sk1.type = com.act.maxc.flume.sinks.kafka.AvroCompactKafkaSink
csvKafka.sinks.sk1.channel = cMem1
csvKafka.sinks.sk1.kafka.bootstrap.servers = h1:9099,h2:9099,h4:9099
csvKafka.sinks.sk1.compactionFormat = json
csvKafka.sinks.sk1.compactionRate = 1
#nanoTime 1000*1000 millionTime
csvKafka.sinks.sk1.waitTime = 30000000000
csvKafka.sinks.sk1.flumeBatchSize = 10000000





